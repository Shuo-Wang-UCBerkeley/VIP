{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data from WRDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/capstone-2024-summer/src/jenna/data_download\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "role\n",
    "\n",
    "# Establish S3 bucket connection\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"capstone-bucket-4-friends\"\n",
    "\n",
    "# Take a look at current dir\n",
    "print(os.getcwd())\n",
    "\n",
    "from file_utilities import s3_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiR3YPwT4baQ",
    "outputId": "f6f0c07a-4967-447a-8e48-8568c0ca34e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wrds\n",
      "  Using cached wrds-3.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /opt/conda/lib/python3.10/site-packages (from wrds) (1.26.4)\n",
      "Requirement already satisfied: packaging<23.3 in /opt/conda/lib/python3.10/site-packages (from wrds) (23.2)\n",
      "Collecting pandas<2.3,>=2.2 (from wrds)\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting psycopg2-binary<2.10,>=2.9 (from wrds)\n",
      "  Using cached psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting scipy<1.13,>=1.12 (from wrds)\n",
      "  Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting sqlalchemy<2.1,>=2 (from wrds)\n",
      "  Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
      "Using cached wrds-3.2.0-py3-none-any.whl (13 kB)\n",
      "Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: sqlalchemy, scipy, psycopg2-binary, pandas, wrds\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.49\n",
      "    Uninstalling SQLAlchemy-1.4.49:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.49\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: SciPy 1.11.4\n",
      "    Uninstalling SciPy-1.11.4:\n",
      "      Successfully uninstalled SciPy-1.11.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.2.2 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\n",
      "jupyter-scheduler 2.5.2 requires sqlalchemy~=1.0, but you have sqlalchemy 2.0.31 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.2 psycopg2-binary-2.9.9 scipy-1.12.0 sqlalchemy-1.4.52 wrds-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (15.0.0)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, pyarrow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 15.0.0\n",
      "    Uninstalling pyarrow-15.0.0:\n",
      "      Successfully uninstalled pyarrow-15.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-common 0.8.3 requires numpy<1.27,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-core 0.8.3 requires numpy<1.27,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-features 0.8.3 requires numpy<1.27,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires numpy<1.27,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires numpy<1.27,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires numpy<1.27,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.2.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires scipy<1.12,>=1.5.4, but you have scipy 1.12.0 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "gluonts 0.13.7 requires numpy~=1.16, but you have numpy 2.0.0 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\n",
      "langchain 0.1.9 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
      "langchain-community 0.0.38 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
      "nptyping 2.4.1 requires numpy<2.0.0,>=1.20.0; python_version >= \"3.8\", but you have numpy 2.0.0 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
      "sagemaker 2.219.0 requires numpy<2.0,>=1.9.0, but you have numpy 2.0.0 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.0.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\n",
      "statsmodels 0.14.1 requires numpy<2,>=1.18, but you have numpy 2.0.0 which is incompatible.\n",
      "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\n",
      "wrds 3.2.0 requires numpy<1.27,>=1.26, but you have numpy 2.0.0 which is incompatible.\n",
      "wrds 3.2.0 requires sqlalchemy<2.1,>=2, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.0.0 pyarrow-16.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:347\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    345\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:1175\u001b[0m, in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m else:\n\u001b[1;32m   1172\u001b[0m     max_rows = get_option(\"display.max_rows\")\n\u001b[1;32m   1174\u001b[0m # when auto-detecting, so width=None and not in ipython front end\n\u001b[0;32m-> 1175\u001b[0m # check whether repr fits horizontal by actually checking\n\u001b[1;32m   1176\u001b[0m # the width of the rendered repr\n\u001b[1;32m   1177\u001b[0m buf = StringIO()\n\u001b[1;32m   1179\u001b[0m # only care about the stuff we'll actually print out\n\u001b[1;32m   1180\u001b[0m # and to_string on entire frame may be expensive\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/format.py:1074\u001b[0m, in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border, table_id, render_links)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:88\u001b[0m, in \u001b[0;36mHTMLFormatter.to_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines):\n\u001b[1;32m     90\u001b[0m         lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:644\u001b[0m, in \u001b[0;36mNotebookFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_style()\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:94\u001b[0m, in \u001b[0;36mHTMLFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[1;32m     97\u001b[0m         by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m215\u001b[39m)  \u001b[38;5;66;03m# ×  # noqa: RUF003\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:267\u001b[0m, in \u001b[0;36mHTMLFormatter._write_table\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<table\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborder_attr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_section\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    263\u001b[0m     indent,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_body(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</table>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:403\u001b[0m, in \u001b[0;36mHTMLFormatter._write_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<thead>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_col_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_row_header(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:383\u001b[0m, in \u001b[0;36mHTMLFormatter._write_col_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m row\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_columns_formatted_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    384\u001b[0m align \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mjustify\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_truncated_horizontally:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/html.py:611\u001b[0m, in \u001b[0;36mNotebookFormatter._get_columns_formatted_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_columns_formatted_values\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;66;03m# only reached with non-Multi Index\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_flat\u001b[49m(include_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_flat'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      cusip  PERMNO  PERMCO  HSHRCD  DLSTCD HTICK                  HCOMNAM  \\\n",
       "0  00130H10   76712   10996      11     100   AES               A E S CORP   \n",
       "1  00206R10   66093   21645      11     100     T              A T & T INC   \n",
       "2  00507V10   79678   12499      11     233   NaN  ACTIVISION BLIZZARD INC   \n",
       "3  00724F10   75510    8476      11     100  ADBE                ADOBE INC   \n",
       "4  00971T10   87299   17300      11     100  AKAM  AKAMAI TECHNOLOGIES INC   \n",
       "\n",
       "  HTSYMBOL  HNAICS HPRIMEXC  ... NUMDEL NUMNDI      BEGDAT      ENDDAT  \\\n",
       "0      AES  221118        N  ...      1     68  06/26/1991  12/29/2023   \n",
       "1        T  517312        N  ...      1      0  02/16/1984  12/29/2023   \n",
       "2     ATVI  513210        Q  ...      1   1968  10/22/1993  10/12/2023   \n",
       "3     ADBE  511210        Q  ...      1   2185  08/13/1986  12/29/2023   \n",
       "4     AKAM  511210        Q  ...      1   1722  10/29/1999  12/29/2023   \n",
       "\n",
       "       BEGPRC      ENDPRC      BEGRET      ENDRET      BEGVOL      ENDVOL  \n",
       "0  06/26/1991  12/29/2023  06/26/1991  12/29/2023  06/26/1991  12/29/2023  \n",
       "1  02/16/1984  12/29/2023  02/16/1984  12/29/2023  02/16/1984  12/29/2023  \n",
       "2  10/22/1993  10/12/2023  10/22/1993  10/12/2023  10/22/1993  10/12/2023  \n",
       "3  08/13/1986  12/29/2023  08/13/1986  12/29/2023  08/13/1986  12/29/2023  \n",
       "4  10/29/1999  12/29/2023  10/29/1999  12/29/2023  10/29/1999  12/29/2023  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_df = pd.read_csv(\"/home/sagemaker-user/capstone-2024-summer/data/security_master.csv\")\n",
    "\n",
    "sp500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76712, 66093, 79678, 75510, 87299)\n"
     ]
    }
   ],
   "source": [
    "sp500_permnos = tuple(sp500_df[\"PERMNO\"].unique())\n",
    "print(sp500_permnos[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Joined CRSP and Compustat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your WRDS username [sagemaker-user]: jennasparks\n",
      "Enter your password: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Create .pgpass file now [y/n]?:  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n",
      "Executing main query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/wrds/sql.py:580: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_df = pd.concat([full_df, chunk])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed. Saving to CSV...\n",
      "Data extraction and merging complete. File saved as CSV.\n",
      "\n",
      "Columns in the merged dataset:\n",
      "Index(['cusip', 'permno', 'permco', 'issuno', 'hexcd', 'hsiccd', 'date',\n",
      "       'bidlo', 'askhi', 'prc',\n",
      "       ...\n",
      "       'dvpsxq', 'mkvaltq', 'prccq', 'prchq', 'prclq', 'adjex', 'naics',\n",
      "       'shrcd', 'exchcd', 'trdstat'],\n",
      "      dtype='object', length=671)\n"
     ]
    }
   ],
   "source": [
    "# Connect to WRDS\n",
    "db = wrds.Connection()\n",
    "\n",
    "# Set date range\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "# Main query\n",
    "main_query = f\"\"\"\n",
    "SELECT a.*, c.*, d.naics, e.shrcd, e.exchcd, e.trdstat\n",
    "FROM crsp.dsf a\n",
    "LEFT JOIN crsp.ccmxpf_linktable b\n",
    "ON a.permno = b.lpermno\n",
    "AND b.linktype IN ('LC', 'LU')\n",
    "AND b.linkprim IN ('P', 'C')\n",
    "AND a.date BETWEEN b.linkdt AND COALESCE(b.linkenddt, '{end_date}')\n",
    "LEFT JOIN LATERAL (\n",
    "    SELECT *\n",
    "    FROM comp.fundq c2\n",
    "    WHERE c2.gvkey = b.gvkey\n",
    "      AND c2.datadate <= a.date\n",
    "      AND c2.datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "    ORDER BY c2.datadate DESC\n",
    "    LIMIT 1\n",
    ") c ON TRUE\n",
    "LEFT JOIN comp.company d\n",
    "ON b.gvkey = d.gvkey\n",
    "LEFT JOIN crsp.dsenames e\n",
    "ON a.permno = e.permno\n",
    "AND a.date BETWEEN e.namedt AND e.nameendt\n",
    "WHERE a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "AND a.permno IN {sp500_permnos}\n",
    "ORDER BY a.permno, a.date\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing main query...\")\n",
    "data = db.raw_sql(main_query)\n",
    "\n",
    "print(\"Query executed. Saving to CSV...\")\n",
    "\n",
    "# # Save to CSV\n",
    "# data.to_csv(\"sp500_crsp_compustat_merged_2018_2023.csv\", index=False)\n",
    "\n",
    "with open(\"sp500_crsp_compustat_merged_2018_2023.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(data.columns)\n",
    "    for row in data.values:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Close the connection\n",
    "db.close()\n",
    "\n",
    "print(\"Data extraction and merging complete. File saved as CSV.\")\n",
    "\n",
    "# Print column names for reference\n",
    "print(\"\\nColumns in the merged dataset:\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/sagemaker-user/capstone-2024-summer/src/jenna/data_download/sp500_crsp_compustat_merged_2018_2023.csv\"\n",
    "# Read the CSV file with specified data types\n",
    "df = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>permno</th>\n",
       "      <th>permco</th>\n",
       "      <th>issuno</th>\n",
       "      <th>hexcd</th>\n",
       "      <th>hsiccd</th>\n",
       "      <th>date</th>\n",
       "      <th>bidlo</th>\n",
       "      <th>askhi</th>\n",
       "      <th>prc</th>\n",
       "      <th>...</th>\n",
       "      <th>costat</th>\n",
       "      <th>fic</th>\n",
       "      <th>cshtrq</th>\n",
       "      <th>dvpspq</th>\n",
       "      <th>dvpsxq</th>\n",
       "      <th>mkvaltq</th>\n",
       "      <th>prccq</th>\n",
       "      <th>prchq</th>\n",
       "      <th>prclq</th>\n",
       "      <th>adjex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>46.170</td>\n",
       "      <td>47.8011</td>\n",
       "      <td>46.63</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>47.440</td>\n",
       "      <td>48.0700</td>\n",
       "      <td>47.71</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>47.715</td>\n",
       "      <td>48.1900</td>\n",
       "      <td>48.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>48.280</td>\n",
       "      <td>48.6300</td>\n",
       "      <td>48.47</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>47.940</td>\n",
       "      <td>49.0700</td>\n",
       "      <td>48.98</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>48.920</td>\n",
       "      <td>49.3600</td>\n",
       "      <td>49.06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>48.580</td>\n",
       "      <td>49.2700</td>\n",
       "      <td>48.80</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>48.440</td>\n",
       "      <td>49.0200</td>\n",
       "      <td>48.95</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>49.060</td>\n",
       "      <td>49.8250</td>\n",
       "      <td>49.51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>68389X10</td>\n",
       "      <td>10104</td>\n",
       "      <td>8045</td>\n",
       "      <td>10536</td>\n",
       "      <td>1</td>\n",
       "      <td>7379</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>49.440</td>\n",
       "      <td>50.0600</td>\n",
       "      <td>49.59</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 667 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cusip  permno  permco  issuno  hexcd  hsiccd        date   bidlo  \\\n",
       "0  68389X10   10104    8045   10536      1    7379  2018-01-02  46.170   \n",
       "1  68389X10   10104    8045   10536      1    7379  2018-01-03  47.440   \n",
       "2  68389X10   10104    8045   10536      1    7379  2018-01-04  47.715   \n",
       "3  68389X10   10104    8045   10536      1    7379  2018-01-05  48.280   \n",
       "4  68389X10   10104    8045   10536      1    7379  2018-01-08  47.940   \n",
       "5  68389X10   10104    8045   10536      1    7379  2018-01-09  48.920   \n",
       "6  68389X10   10104    8045   10536      1    7379  2018-01-10  48.580   \n",
       "7  68389X10   10104    8045   10536      1    7379  2018-01-11  48.440   \n",
       "8  68389X10   10104    8045   10536      1    7379  2018-01-12  49.060   \n",
       "9  68389X10   10104    8045   10536      1    7379  2018-01-16  49.440   \n",
       "\n",
       "     askhi    prc  ...  costat  fic  cshtrq  dvpspq  dvpsxq  mkvaltq  prccq  \\\n",
       "0  47.8011  46.63  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "1  48.0700  47.71  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "2  48.1900  48.18  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "3  48.6300  48.47  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "4  49.0700  48.98  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "5  49.3600  49.06  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "6  49.2700  48.80  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "7  49.0200  48.95  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "8  49.8250  49.51  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "9  50.0600  49.59  ...     NaN  NaN     NaN     NaN     NaN      NaN    NaN   \n",
       "\n",
       "   prchq  prclq  adjex  \n",
       "0    NaN    NaN    NaN  \n",
       "1    NaN    NaN    NaN  \n",
       "2    NaN    NaN    NaN  \n",
       "3    NaN    NaN    NaN  \n",
       "4    NaN    NaN    NaN  \n",
       "5    NaN    NaN    NaN  \n",
       "6    NaN    NaN    NaN  \n",
       "7    NaN    NaN    NaN  \n",
       "8    NaN    NaN    NaN  \n",
       "9    NaN    NaN    NaN  \n",
       "\n",
       "[10 rows x 667 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Raw Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    \"/home/sagemaker-user/capstone-2024-summer/src/jenna/data_download/new_crsp_compustat_merged_2018_2023.csv\",\n",
    "    bucket,\n",
    "    \"CRSP/new_crsp_compustat_merged_2018_2023.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/new_crsp_compustat_merged_2018_2023.csv.74B39744'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reload_path \u001b[38;5;241m=\u001b[39m \u001b[43ms3_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRSP/new_crsp_compustat_merged_2018_2023.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/capstone-2024-summer/src/jenna/file_utilities.py:54\u001b[0m, in \u001b[0;36ms3_download\u001b[0;34m(s3_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m target_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(target_path)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(target_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/boto3/s3/inject.py:192\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/boto3/s3/transfer.py:405\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    401\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m    402\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[1;32m    403\u001b[0m )\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/tasks.py:139\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# If the task is not done (really only if some other related\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# task to the TransferFuture had failed) then execute the task's\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# main() method.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/tasks.py:162\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Log what is about to be executed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_final:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/download.py:642\u001b[0m, in \u001b[0;36mIOWriteTask._main\u001b[0;34m(self, fileobj, data, offset)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_main\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileobj, data, offset):\n\u001b[1;32m    636\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pulls off an io queue to write contents to a file\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    :param fileobj: The file handle to write content to\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    :param data: The data to write\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    :param offset: The offset to write the data to.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/utils.py:387\u001b[0m, in \u001b[0;36mDeferredOpenFile.seek\u001b[0;34m(self, where, whence)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseek\u001b[39m(\u001b[38;5;28mself\u001b[39m, where, whence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mseek(where, whence)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/utils.py:370\u001b[0m, in \u001b[0;36mDeferredOpenFile._open_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_if_needed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_byte \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_byte)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3transfer/utils.py:281\u001b[0m, in \u001b[0;36mOSUtils.open\u001b[0;34m(self, filename, mode)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, mode):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/new_crsp_compustat_merged_2018_2023.csv.74B39744'"
     ]
    }
   ],
   "source": [
    "reload_path = s3_download(\"CRSP/new_crsp_compustat_merged_2018_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_169/76956656.py:1: DtypeWarning: Columns (0,21,25,26,27,28,29,30,31,32,33,37,38,39,40,43,44,45,46,47,52,53,54,55,657,658) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reload = pd.read_csv(\"/home/sagemaker-user/capstone-2024-summer/src/jenna/data_download/sp500_crsp_compustat_merged_2018_2023.csv\")\n"
     ]
    }
   ],
   "source": [
    "reload = pd.read_csv(\"/home/sagemaker-user/capstone-2024-summer/src/jenna/data_download/sp500_crsp_compustat_merged_2018_2023.csv\")\n",
    "assert df.equals(reload)\n",
    "\n",
    "reload.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
